{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentandoDatasets import segmentando_datasets\n",
    "segmentando_datasets(1000,1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessamento import *\n",
    "\n",
    "csv_file = 'Datasets_csv/df_PUC.csv'\n",
    "train, teste, val, _, _, _ = preprocessamento(csv_file, 0.6, 0.2, 0.2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.dtype)\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"x_train = next(train)\n",
    "\n",
    "diff = x_train[0][0] - x_train[1][0]\n",
    "print(np.sum(diff))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(train)\n",
    "\n",
    "fig, axes = plt.subplots(1, 16, figsize=(15, 5))\n",
    "\n",
    "for i in range(16):\n",
    "    axes[i].imshow(np.clip(images[i], 0, 255).astype('uint8'))\n",
    "    axes[i].axis('off') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data_iterator):\n",
    "        self.data_iterator = data_iterator\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_iterator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.data_iterator[idx]\n",
    "        if isinstance(batch, tuple):\n",
    "            return (batch[0] / 255.0, batch[0] / 255.0)  # Normalize input and target\n",
    "        else:\n",
    "            return batch / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, input_shape=(256, 256, 3)):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=input_shape),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(128, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2DTranspose(3, kernel_size=3, strides=2, activation='sigmoid', padding='same')\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 10:\n",
    "        lr *= 0.1\n",
    "    return lr\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\"\"\"autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                    loss='mse')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, callbacks\n",
    "\n",
    "\n",
    "checkpoint_path = 'weights_parciais/weights-improvement-{epoch:02d}-{loss:.2f}.weights.h5'\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                  save_weights_only=True, \n",
    "                                  monitor='loss',\n",
    "                                  mode='max', \n",
    "                                  save_best_only=True, \n",
    "                                  verbose=1)\n",
    "\n",
    "\"\"\"cp_callback = callbacks.ModelCheckpoint(filepath='best_model.keras',\n",
    "                                        save_best_only=True,\n",
    "                                        monitor='val_loss')\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=10,\n",
    "                                         restore_best_weights=True)\n",
    "lr_scheduler = callbacks.LearningRateScheduler(lr_schedule)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_gen = NormalizedDataGenerator(train)\n",
    "val_gen = NormalizedDataGenerator(val)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(train,\n",
    "                          epochs=50,\n",
    "                          validation_data=val,\n",
    "                          callbacks=[cp_callback],\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss (Log Scale)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('Modelos_keras/Autoencoder2.keras')\n",
    "autoencoder.save_weights('weights_finais/Autoencoder2.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights('weights_finais/Autoencoder2.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(train[0])\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_imgs.shape)\n",
    "print(decoded_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = next(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(decoded_imgs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "input_imgs, _ = train[0]  # Extraia as imagens\n",
    "\n",
    "num_imgs = min(encoded_imgs.shape[0], 5)  # Por exemplo, visualize no máximo 5 imagens\n",
    "num_channels = encoded_imgs.shape[-1]  # Número de canais\n",
    "num_channels_to_show = min(num_channels, 10)  # Número de canais a mostrar por imagem\n",
    "\n",
    "fig, axes = plt.subplots(3, num_imgs, figsize=(15, 9))\n",
    "\n",
    "for i in range(num_imgs):\n",
    "    axes[0, i].imshow(np.clip(input_imgs[i], 0, 255).astype('uint8'))\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title('Original')\n",
    "\n",
    "for i in range(num_imgs):\n",
    "    axes[1, i].imshow(np.clip(decoded_imgs[i], 0, 255).astype('uint8'))\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title('Decodificado')\n",
    "\n",
    "for i in range(num_imgs):\n",
    "    encoded_img = encoded_imgs[i]\n",
    "    for j in range(num_channels_to_show):\n",
    "        axes[2, i].imshow(encoded_img[:, :, j], cmap='gray')\n",
    "        axes[2, i].axis('off')\n",
    "        axes[2, i].set_title(f'Codificada')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UFPR04, _ = preprocessamento_dataframe_unico(\"Datasets_csv/df_UFPR04.csv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"x_UFPR04, _ = next(UFPR04)\n",
    "\n",
    "x_UFPR04 = x_UFPR04.astype('float32')/255.\n",
    "\n",
    "print(x_UFPR04.shape)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
