{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 3.5.0\n",
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = tf.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.tf.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 72 X 72\n",
      "Patch size: 16 X 16\n",
      "Patches per image: 16\n",
      "Elements per patch: 768\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_patches(image, patch_size):\n",
    "    \"\"\" Extrai patches quadrados de uma imagem. \"\"\"\n",
    "    patches = []\n",
    "    h, w, _ = image.shape\n",
    "    for i in range(0, h - patch_size + 1, patch_size):\n",
    "        for j in range(0, w - patch_size + 1, patch_size):\n",
    "            patch = image[i:i + patch_size, j:j + patch_size]\n",
    "            patches.append(patch)\n",
    "    return np.array(patches)\n",
    "\n",
    "image_size = 72  # Tamanho da imagem\n",
    "patch_size = 16  # Tamanho do patch\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "resized_image = cv2.resize(image, (image_size, image_size))\n",
    "\n",
    "patches = extract_patches(resized_image, patch_size)\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[0]}\")\n",
    "print(f\"Elements per patch: {patches.shape[1] * patches.shape[2] * patches.shape[3]}\")\n",
    "\n",
    "# Exibe os patches\n",
    "n = int(np.sqrt(patches.shape[0]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    plt.imshow(patch.astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = tf.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "\n",
    "        # Compute number of patches\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        num_patches = num_patches_h * num_patches_w\n",
    "        patch_height = self.patch_size\n",
    "        patch_width = self.patch_size\n",
    "\n",
    "        # Extract patches\n",
    "        patches = tf.image.extract_patches(\n",
    "            images,\n",
    "            sizes=[1, patch_height, patch_width, 1],\n",
    "            strides=[1, patch_height, patch_width, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "        \n",
    "        # Reshape patches to [batch_size, num_patches, patch_height * patch_width * channels]\n",
    "        patch_dim = patch_height * patch_width * channels\n",
    "        patches = tf.reshape(patches, [batch_size, num_patches, patch_dim])\n",
    "        return patches\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        num_patches = num_patches_h * num_patches_w\n",
    "        patch_height = self.patch_size\n",
    "        patch_width = self.patch_size\n",
    "\n",
    "        patch_dim = patch_height * patch_width * channels\n",
    "        return (batch_size, num_patches, patch_dim)\n",
    "    \n",
    "    import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.expand_dims(\n",
    "            tf.range(start=0, limit=self.num_patches, delta=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/PIBIC/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:1383: UserWarning: Layer 'patch_encoder' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Dimensions must be equal, but are 16 and 144 for '{{node add}} = AddV2[T=DT_FLOAT](dense_2_1/Add, embedding_1/GatherV2)' with input shapes: [?,16,64], [1,144,64].''\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling PatchEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'patch_encoder' (of type PatchEncoder). Either the `PatchEncoder.call()` method is incorrect, or you need to implement the `PatchEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 16 and 144 for '{{node add}} = AddV2[T=DT_FLOAT](dense_2_1/Add, embedding_1/GatherV2)' with input shapes: [?,16,64], [1,144,64].\u001b[0m\n\nArguments received by PatchEncoder.call():\n  • args=('<KerasTensor shape=(None, 16, 768), dtype=float32, sparse=False, name=keras_tensor_34>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest top 5 accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(top_5_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[0;32m---> 40\u001b[0m vit_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_vit_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m history \u001b[38;5;241m=\u001b[39m run_experiment(vit_classifier)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_history\u001b[39m(item):\n",
      "Cell \u001b[0;32mIn[36], line 8\u001b[0m, in \u001b[0;36mcreate_vit_classifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m patches \u001b[38;5;241m=\u001b[39m Patches(patch_size)(augmented)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Encode patches.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m encoded_patches \u001b[38;5;241m=\u001b[39m \u001b[43mPatchEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create multiple layers of the Transformer block.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(transformer_layers):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Layer normalization 1.\u001b[39;00m\n",
      "File \u001b[0;32m~/PIBIC/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[35], line 69\u001b[0m, in \u001b[0;36mPatchEncoder.call\u001b[0;34m(self, patch)\u001b[0m\n\u001b[1;32m     65\u001b[0m positions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(\n\u001b[1;32m     66\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_patches, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m projected_patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(patch)\n\u001b[0;32m---> 69\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[43mprojected_patches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling PatchEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'patch_encoder' (of type PatchEncoder). Either the `PatchEncoder.call()` method is incorrect, or you need to implement the `PatchEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 16 and 144 for '{{node add}} = AddV2[T=DT_FLOAT](dense_2_1/Add, embedding_1/GatherV2)' with input shapes: [?,16,64], [1,144,64].\u001b[0m\n\nArguments received by PatchEncoder.call():\n  • args=('<KerasTensor shape=(None, 16, 768), dtype=float32, sparse=False, name=keras_tensor_34>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)\n",
    "\n",
    "\n",
    "def plot_history(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(\"loss\")\n",
    "plot_history(\"top-5-accuracy\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
